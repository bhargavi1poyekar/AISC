{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AISC_Activ_fun.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94aDnXpNn_K4"
      },
      "source": [
        "### Name: Bhargavi Poyekar\n",
        "### BE COMPS - Batch C\n",
        "### UID: 2018130040"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0O60P1vscaP"
      },
      "source": [
        "# AISC LAB 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8x9MFohFsdPj"
      },
      "source": [
        "## AIM: To implement Transfer/Activation Functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZ_nDQDKg8SP"
      },
      "source": [
        "## THEORY:\n",
        "\n",
        "An artificial neuron calculates the ‘weighted sum’ of its inputs and adds a bias. Now the value of net input can be any anything from -inf to +inf. The neuron doesn’t really know how to bound to value and thus is not able to decide the firing pattern. Thus the activation function is an important part of an artificial neural network. They basically decide whether a neuron should be activated or not. Thus it bounds the value of the net input.\n",
        "The activation function is a non-linear transformation that we do over the input before sending it to the next layer of neurons or finalizing it as output.\n",
        "\n",
        "Types of Activation Functions –\n",
        "Several different types of activation functions are used in Deep Learning. \n",
        "\n",
        "1. **Step Function**:\n",
        "Step Function is one of the simplest kind of activation functions. In this, we consider a threshold value and if the value of net input say y is greater than the threshold then the neuron is activated.\n",
        "\n",
        "2. **Sigmoid Function**:\n",
        "Sigmoid function is a widely used activation function. This is a smooth function and is continuously differentiable. The biggest advantage that it has over step and linear function is that it is non-linear. This is an incredibly cool feature of the sigmoid function. This essentially means that when I have multiple neurons having sigmoid function as their activation function – the output is non linear as well. The function ranges from 0-1 having an S shape.\n",
        "\n",
        "3. **ReLU**:\n",
        "The ReLU function is the Rectified linear unit. It is the most widely used activation function. The main advantage of using the ReLU function over other activation functions is that it does not activate all the neurons at the same time.  If you look at the ReLU function if the input is negative it will convert it to zero and the neuron does not get activated.\n",
        "\n",
        "4. **Tanh Function** :- The activation that works almost always better than sigmoid function is Tanh function also knows as Tangent Hyperbolic function. It’s actually mathematically shifted version of the sigmoid function. Both are similar and can be derived from each other.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qniLNl7Wsf8q"
      },
      "source": [
        "## CODE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxtwzQyjs8K8"
      },
      "source": [
        "# import libraries\n",
        "import math\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVnK__3grjLg"
      },
      "source": [
        "### Binary sigmoidal activation function "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVGhvNzUrgl5"
      },
      "source": [
        "def bin_sigm(yin):\n",
        "  print(\"Binary Sigmoidal Activation function: \")\n",
        "  eyin=math.exp(-yin)\n",
        "  y=1/(1+eyin)\n",
        "  return (round(y,3))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVvJkCMUufaT"
      },
      "source": [
        "### Bipolar sigmoidal activation function "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3xzpk3sujwU"
      },
      "source": [
        "def bip_sigm(yin):\n",
        "  print(\"Bipolar Sigmoidal Activation function: \")\n",
        "  eyin=math.exp(-yin)\n",
        "  y=(2/(1+eyin))-1\n",
        "  return (round(y,3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sv7hXA4qv50R"
      },
      "source": [
        "### Tangent Hyperbolic function (tanh)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIRCOkiOwCZh"
      },
      "source": [
        "def tanh(yin):\n",
        "  print(\"Tangent Hyperbolic function: \")\n",
        "  eyin=math.exp(-2*yin)\n",
        "  y=(2/(1+eyin))-1\n",
        "  return (round(y,3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94ZHtF_IxE7B"
      },
      "source": [
        "### ReLU function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMI-n0LNxKE8"
      },
      "source": [
        "def ReLU(yin):\n",
        "  print(\"ReLU function: \")\n",
        "  return max(0,yin)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4HUFhqL1Gad"
      },
      "source": [
        "### Ramp function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWgk-hUl1J-G"
      },
      "source": [
        "def Ramp(yin):\n",
        "  if yin>1:\n",
        "    return 1\n",
        "  elif yin<=1 and yin >= 0:\n",
        "    return yin\n",
        "  return 0 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cm3CXSCNu5ar"
      },
      "source": [
        "### Input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPQeC5ewnlOz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4f14cdd-99da-4b09-e832-370e9800e830"
      },
      "source": [
        "input_val=list(map(float, input(\"Enter the input values for network: \").split()))\n",
        "weights=list(map(float, input(\"Enter the weights: \").split()))\n",
        "bias=float(input(\"Enter the value of bias: \"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the input values for network: 0.8 0.6 0.4\n",
            "Enter the weights: 0.1 0.3 -0.2\n",
            "Enter the value of bias: 0.35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNw_p3G4vE0w"
      },
      "source": [
        "### Calculate Net input (yin) and calling activation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7yS_I1_qX8X",
        "outputId": "90b998c9-222b-4175-f305-ee178b376ac7"
      },
      "source": [
        "sum=0\n",
        "for i in range(len(input_val)):\n",
        "  sum+=input_val[i]*weights[i]  \n",
        "\n",
        "yin=bias+sum\n",
        "print(f'yin = {yin}')\n",
        "f=1\n",
        "\n",
        "while f: \n",
        "  ch=int(input(\"\\nSelect the activation function: \\n1. Binary Sigmoidal Activation Function \\n2. Bipolar sigmoidal activation function\\n3. Tangent Hyperbolic Function \\n4. ReLU Function \\n5. Ramp function \\n6. Exit\\n\"))\n",
        "  if ch==1:\n",
        "    y = bin_sigm(yin)\n",
        "  elif ch==2:\n",
        "    y = bip_sigm(yin)\n",
        "  elif ch==3:\n",
        "    y=tanh(yin)\n",
        "  elif ch==4:\n",
        "    y=ReLU(yin)\n",
        "  elif ch==5:\n",
        "    y=Ramp(yin)\n",
        "  else:\n",
        "    f=0\n",
        "    break\n",
        "  print(f'Ouput y = f(yin) = {y}')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "yin = 0.53\n",
            "\n",
            "Select the activation function: \n",
            "1. Binary Sigmoidal Activation Function \n",
            "2. Bipolar sigmoidal activation function\n",
            "3. Tangent Hyperbolic Function \n",
            "4. ReLU Function \n",
            "5. Ramp function \n",
            "6. Exit\n",
            "1\n",
            "Binary Sigmoidal Activation function: \n",
            "Ouput y = f(yin) = 0.629\n",
            "\n",
            "Select the activation function: \n",
            "1. Binary Sigmoidal Activation Function \n",
            "2. Bipolar sigmoidal activation function\n",
            "3. Tangent Hyperbolic Function \n",
            "4. ReLU Function \n",
            "5. Ramp function \n",
            "6. Exit\n",
            "2\n",
            "Bipolar Sigmoidal Activation function: \n",
            "Ouput y = f(yin) = 0.259\n",
            "\n",
            "Select the activation function: \n",
            "1. Binary Sigmoidal Activation Function \n",
            "2. Bipolar sigmoidal activation function\n",
            "3. Tangent Hyperbolic Function \n",
            "4. ReLU Function \n",
            "5. Ramp function \n",
            "6. Exit\n",
            "3\n",
            "Tangent Hyperbolic function: \n",
            "Ouput y = f(yin) = 0.485\n",
            "\n",
            "Select the activation function: \n",
            "1. Binary Sigmoidal Activation Function \n",
            "2. Bipolar sigmoidal activation function\n",
            "3. Tangent Hyperbolic Function \n",
            "4. ReLU Function \n",
            "5. Ramp function \n",
            "6. Exit\n",
            "4\n",
            "ReLU function: \n",
            "Ouput y = f(yin) = 0.53\n",
            "\n",
            "Select the activation function: \n",
            "1. Binary Sigmoidal Activation Function \n",
            "2. Bipolar sigmoidal activation function\n",
            "3. Tangent Hyperbolic Function \n",
            "4. ReLU Function \n",
            "5. Ramp function \n",
            "6. Exit\n",
            "5\n",
            "Ouput y = f(yin) = 0.53\n",
            "\n",
            "Select the activation function: \n",
            "1. Binary Sigmoidal Activation Function \n",
            "2. Bipolar sigmoidal activation function\n",
            "3. Tangent Hyperbolic Function \n",
            "4. ReLU Function \n",
            "5. Ramp function \n",
            "6. Exit\n",
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gY4S9dGDjNqd"
      },
      "source": [
        "## CONCLUSION:\n",
        "\n",
        "1. I understood what are activation functions and why do we need it.\n",
        "2. I also learned different types of activation functions and how to implement them.\n",
        "\n"
      ]
    }
  ]
}